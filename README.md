Memory mapped Memory

for large language models (llm's) the context size (the number of tokens the llm can look into the passt) is the main cost for training and 
the main limitation. We test a way to make this context infinite by using an old idea 'memory mapped io'.
