% ------------------------------------------------
% LaTeX Template for ML/AI Research Paper
% ------------------------------------------------

\documentclass[11pt]{article}

% ------------------------------------------------
% Packages
% ------------------------------------------------
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{natbib}          % For bibliography
\usepackage{geometry}        % For adjusting margins
\usepackage{times}           % For Times New Roman font
\usepackage{float}           % For controlling float positions
\usepackage{booktabs}        % For professional-looking tables
\usepackage{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage{tikz}

% ------------------------------------------------
% Page Setup
% ------------------------------------------------
\geometry{margin=1in}

% ------------------------------------------------
% Title and Author Information
% ------------------------------------------------
\title{Memory mapped Memory for transformers}
\author{
  Bastian Apportin\thanks{Corresponding author: \texttt{bastian.apportin@joomo.de}} \\
  \normalsize Joomo GmbH \\
%  \and
%  Co-author Name \\
%  \normalsize Co-author Affiliation \\
%  % Add more authors if necessary
}
\date{\today}

% ------------------------------------------------
% Document Begins
% ------------------------------------------------
\begin{document}

\maketitle

% ------------------------------------------------
% Abstract
% ------------------------------------------------
\begin{abstract}
% Provide a concise summary of the entire paper (150-250 words).
% Include background, methods, results, and conclusions.
The success of LLM's is mainly based on the astonishing capabilities of the Transformer Architecture. 
Which does a great job in catching long range dependencies.

The positional encoding used in Transformers solves the problem of conbtext awarness, but limits the distance a transformer can look. What we want is aktually 
to keep the context awarness of the architecture, like distance memories coming back, influencing your current thoughts.

We've tried a lot of different aproaches like ALiBi or placing cnn's in the key/query calculations, but none of them worked like we expected. All the limited
experiments we did worked significantly better when using RoPE like positin encodings. But maybe, we don't need to remove them.

The RoPE embedding can be seen as an offset plus content adressing. The offset is the rotation and the mixture of the frequencies can adress sharper or broader position
back from the current position. 



\end{abstract}
% Optionally, you can add keywords
% \vspace{0.5cm}
% \noindent\textbf{Keywords:} Keyword1, Keyword2, Keyword3

% ------------------------------------------------
% Main Content
% ------------------------------------------------

% 1. Introduction
\section{Introduction}
% Introduce the topic, establish the importance, and state the main contributions.

The transformer architecture is self attention as its core. The worst and best part of that is, that there is no local context. Therefore 
most Transformer architectures apply some kind of positional encoding. This positional encoding solves the problem of the missing local context
but introduces the problem that for long distances the 'context' or distance matters as well. Therefore the view of site can not be infinite, which 
effectivly limits memorizing things by that mechanism.

The RoPE embedding can be seen as an offset plus content adressing. The offset is the rotation and the mixture of the frequencies can adress sharper or broader position ranges
back from the current position and the content are the original values of the queries and keys. 

% - Context

When we have positional adressing an idea came to mind, the memory mapped IO, where registers to access secondary hardware are mapped to main memory. One would write the block
id of a harddrive to a memory position and then the content of that block will be mapped to a specific position in memory.



% - Problem Statement
% - Objectives
% - Contributions
% - Structure of the paper



% 2. Methodology
\section{Methodology}

we use the llama 3.2 1b model for our experiment, since that is a size we can still handle.

the llama model uses RoPE. RoPE implements an influence of relative distance. So it's not important what actual position(id) the
token has. The result will be the same if we shift the wohle sequence.

We think of the current position as the anchor and place the mapped memories at a fixed negative offset. For ease of understanding
we place the current position at 4096 and the memory map at 0. This way we can store the unrotated keys and query them using a kind
of LSH. The algorithm is an aproximation and the model ist not trained for that explicitly. We'll see if that actually works.

We can now store unrotated older key,value pairs in an LSH like strukture and search with the rotated queries. The queried results 
can be appended to the key and value tensors before the attention calculation. This way relevant memories from the storage can
'come to mind' and influence the generated sequence.

This mechanism can probably improved when the model is trained (another fine tuning step maybe) with that mechanism in place. In 
addition this mechanism can be used to memory map other mechanisms. For example we have a transformer model that generates limb 
movements from camera inputs and an llm to understand and generate text. We can now memory map those models into each other - potentially
with a linear transformation in between. So the text model has a limited view on some key,values from the vision/limb model and vise versa.

For now we want to place memories in the storage and ask questions where the answer can show that knowledge from the memory was aktually used.

%- use llama 3.2 1b
%- explain using rotatry embeddings with fixed 'current position'
%- map lsh like storage to defined position 0
%- search by unrotated query
%- provide a way to place memories in that storage
%- use questions that indicate that the memories where used 

%\begin{figure}[h]
%\centering
%\includegraphics[scale=0.2]{JoMoTra.jpg}
%\caption{Architecture}
%\label{fig:Architecture}
%\end{figure}

% - Theoretical Background
% - Model Architecture
%\begin{tikzpicture}
%\draw (0,0) -- (4,0) -- (3,2) -- (1,2) -- (0,0);
%\end{tikzpicture}
% - Algorithms
% - Assumptions
% - Implementation Details

% 3. Experiments
\section{Experiments}

To do useful experiments we will use fairy tales that we will put to the memory and a set of commands we ask the model
like versions of 'tell me a story about ...' 

for the dataset we use the 

\begin{itemize}
\item https://www.kaggle.com/datasets/cuddlefish/fairy-tales/data
\end{itemize}

dataset from kaggle. 

For the Questions we asked chatgpt to produce context free questions that would be influenced when reading one of the fairy tailes before. The resulting Questions:

\begin{itemize}
\item What kind of person do you think is most likely to succeed in a difficult situation?
\begin{itemize}
\item A reader of To Your Good Health! might say “a trickster,” while Lovely Ilonka might lead to “a brave and clever person,” and Lucky Luck could prompt “someone with good fortune.”
\end{itemize}

\item If someone has a secret, should they always tell it? Why or why not?
\begin{itemize}
\item Answers would differ strongly between The Boy Who Could Keep a Secret, The Language of Beasts, or someone who hasn't read any tale.
\end{itemize}

\item Do you think it’s better to be lucky or smart?
\begin{itemize}
\item A Lucky Luck reader may favor luck, while The Hairy Man or Seven Simons might push someone toward cleverness or teamwork.
\end{itemize}

\item What kinds of help are most valuable — magical, practical, or social?
\begin{itemize}
\item A reader of The Seven Simons may argue for teamwork (practical), while Lovely Ilonka or The Hairy Man could lead someone to defend magical help.
\end{itemize}

\item Is silence ever more powerful than speaking?
\begin{itemize}
\item The Language of Beasts or The Boy Who Could Keep a Secret readers might say yes; others may disagree.
\end{itemize}

\item How should someone deal with people in power who are being unfair?
\begin{itemize}
\item Readers of To Your Good Health! may lean toward rebellion or trickery, while others might suggest patience, service, or outwitting.
\end{itemize}

\item If you could have one unusual skill, what would it be and why?
\begin{itemize}
\item The Seven Simons readers might recall specific skills (e.g., building towers, spying from afar) and answer accordingly.
\end{itemize}

\item What makes someone truly trustworthy?
\begin{itemize}
\item Answers might reflect how characters like the boy in The Hairy Man or The Boy Who Could Keep a Secret prove themselves.
\end{itemize}

\item Do you believe people can change their fate? Why or why not?
\begin{itemize}
\item Some tales lean toward predestination (Lucky Luck), others toward personal effort (Lovely Ilonka, The Hairy Man).
\end{itemize}

\item Can disobedience ever be a good thing?
\begin{itemize}
\item Characters in The Hairy Man or The Language of Beasts disobey to their benefit — this could spark a nuanced response from someone who read those stories.
\end{itemize}

\end{itemize}

we will use this question for now together with the fairy tails (Lovely Ilonka, Lucky Luck, The Hairy Man, To Your Good Health!, The Story of the Seven Simons, The Language of Beasts or The Boy Who Could Keep A Secret) or none and see if the generated answers lead to results that depend on the memory. 


%- define set of memories to use
%- define set of questions, that use this memories.

% Describe how you tested your model and ensure reproducibility.
% - Datasets
% - Experimental Setup
% - Hyperparameters
%parameters: 92793376, conv1d(16,32,kernel=7) conv1d(32,512,kernel=7) 32*block_layer(hidden_size=512 ) conv1d(512,258,kernel=3)
%parameters: 400096, conv1d(16,32,kernel=7) conv1d(32,32,kernel=7) 32*block_layer(hidden_size=32 ) conv1d(32,258,kernel=3) loss=3.6
% - Evaluation Metrics

% 4. Results
\section{Results}
% Present the findings of your experiments.
% - Quantitative Results (Use tables and figures)
% - Qualitative Results
% - Comparisons
% - Statistical Analysis

% Example of including a figure
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.7\textwidth]{path/to/your/image.png}
%     \caption{Caption of the figure.}
%     \label{fig:label}
% \end{figure}

% Example of including a table
% \begin{table}[H]
%     \centering
%     \caption{Caption of the table.}
%     \label{tab:label}
%     \begin{tabular}{lcc}
%         \toprule
%         Header1 & Header2 & Header3 \\
%         \midrule
%         Row1 & Data & Data \\
%         Row2 & Data & Data \\
%         \bottomrule
%     \end{tabular}
% \end{table}

% 5. Discussion
\section{Discussion}
% Interpret the results and discuss their implications.
% - Insights
% - Limitations
% - Practical Implications
% - Theoretical Implications

Using this memory mapping mechanism one could not only access secondary storage, but other systems as well. For example a transformer (maybe in combination with cnn's) based system that take camera input and produces limb movements. 

% 6. Conclusion
\section{Conclusion}
% Summarize the research and suggest future directions.




% - Summary of findings
% - Significance
% - Future Work

% 7. Related Work
\section{Related Work}
% Situate your research within the existing body of work.
% - Literature Review
% - Critical Analysis
% - Positioning

% 8. Acknowledgments
\section*{Acknowledgments}
% Credit those who contributed to the research but are not listed as authors.
% - Funding Sources
% - Collaborations

% ------------------------------------------------
% References
% ------------------------------------------------
\bibliographystyle{plainnat}
\bibliography{references}

% ------------------------------------------------
% Appendices (Optional)
% ------------------------------------------------
\appendix

\section{Appendix Title}
% Include supplementary material that supports the paper but is too detailed for the main text.
% - Mathematical Proofs
% - Additional Figures or Tables
% - Code Listings

% Example of an equation
% \begin{equation}
%     E = mc^2
% \end{equation}

% ------------------------------------------------
% Document Ends
% ------------------------------------------------
\end{document}
